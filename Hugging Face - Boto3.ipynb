{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb2622d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker>=2.48.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (2.69.0)\n",
      "Requirement already satisfied: transformers==4.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (4.6.1)\n",
      "Requirement already satisfied: datasets[s3]==1.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers==4.6.1) (1.21.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers==4.6.1) (0.10.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers==4.6.1) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers==4.6.1) (4.49.0)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers==4.6.1) (0.0.46)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers==4.6.1) (21.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers==4.6.1) (4.6.3)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers==4.6.1) (0.0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers==4.6.1) (2021.8.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from transformers==4.6.1) (2.26.0)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets[s3]==1.6.2) (0.3.4)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets[s3]==1.6.2) (0.70.12.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets[s3]==1.6.2) (1.3.1)\n",
      "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets[s3]==1.6.2) (5.0.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets[s3]==1.6.2) (2021.7.0)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets[s3]==1.6.2) (2.0.2)\n",
      "Collecting botocore==1.19.52\n",
      "  Using cached botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Collecting boto3==1.16.43\n",
      "  Using cached boto3-1.16.43-py2.py3-none-any.whl (130 kB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from boto3==1.16.43->datasets[s3]==1.6.2) (0.10.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (1.26.6)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.2.8)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sagemaker>=2.48.0) (21.2.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sagemaker>=2.48.0) (3.17.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from importlib-metadata->transformers==4.6.1) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from importlib-metadata->transformers==4.6.1) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from packaging->transformers==4.6.1) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from protobuf>=3.1->sagemaker>=2.48.0) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from requests->transformers==4.6.1) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from requests->transformers==4.6.1) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from requests->transformers==4.6.1) (2.0.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from pandas->datasets[s3]==1.6.2) (2021.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.3.0)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from s3fs->datasets[s3]==1.6.2) (1.2.2)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2021.4.0-py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: aiohttp>=3.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.7.4.post0)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (0.8.0)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.12.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (5.1.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (4.0.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.6.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sacremoses->transformers==4.6.1) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from sacremoses->transformers==4.6.1) (8.0.1)\n",
      "Installing collected packages: botocore, s3transfer, fsspec, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.23.11\n",
      "    Uninstalling botocore-1.23.11:\n",
      "      Successfully uninstalled botocore-1.23.11\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.5.0\n",
      "    Uninstalling s3transfer-0.5.0:\n",
      "      Successfully uninstalled s3transfer-0.5.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.7.0\n",
      "    Uninstalling fsspec-2021.7.0:\n",
      "      Successfully uninstalled fsspec-2021.7.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.20.11\n",
      "    Uninstalling boto3-1.20.11:\n",
      "      Successfully uninstalled boto3-1.20.11\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.22.11 requires botocore==1.23.11, but you have botocore 1.19.52 which is incompatible.\n",
      "awscli 1.22.11 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.3.7 which is incompatible.\u001b[0m\n",
      "Successfully installed boto3-1.16.43 botocore-1.19.52 fsspec-2021.4.0 s3transfer-0.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3daf8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::086613482928:role/service-role/AmazonSageMaker-ExecutionRole-20200122T153461\n",
      "sagemaker bucket: sagemaker-us-west-2-086613482928\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a218b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7affae74",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `imdb` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [imdb](http://ai.stanford.edu/~amaas/data/sentiment/) dataset consists of 25000 training and 25000 testing highly polar movie reviews.\n",
    "\n",
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0568221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2bc697ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n",
      "Reusing dataset imdb (/home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7e08f1aa5b420286237d7e334c8583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242a549121ff445797d4fe45cbc8121a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d610519",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e38ddbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a85c5284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (1.16.43)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.20.12-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: awscli in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (1.22.11)\n",
      "Collecting awscli\n",
      "  Downloading awscli-1.22.12-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 37.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: botocore in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (1.19.52)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.23.12-py3-none-any.whl (8.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.2 MB 104.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from boto3) (0.10.0)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Using cached s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: colorama<0.4.4,>=0.2.5 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from awscli) (0.4.3)\n",
      "Requirement already satisfied: PyYAML<5.5,>=3.10 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from awscli) (5.4.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from awscli) (0.15.2)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from botocore) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from botocore) (1.26.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Installing collected packages: botocore, s3transfer, boto3, awscli\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.19.52\n",
      "    Uninstalling botocore-1.19.52:\n",
      "      Successfully uninstalled botocore-1.19.52\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.3.7\n",
      "    Uninstalling s3transfer-0.3.7:\n",
      "      Successfully uninstalled s3transfer-0.3.7\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.16.43\n",
      "    Uninstalling boto3-1.16.43:\n",
      "      Successfully uninstalled boto3-1.16.43\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.22.11\n",
      "    Uninstalling awscli-1.22.11:\n",
      "      Successfully uninstalled awscli-1.22.11\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 1.2.2 requires botocore<1.19.53,>=1.19.52, but you have botocore 1.23.12 which is incompatible.\u001b[0m\n",
      "Successfully installed awscli-1.22.12 boto3-1.20.12 botocore-1.23.12 s3transfer-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 awscli botocore --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "83f90b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c856b5b",
   "metadata": {},
   "source": [
    "# Steps Automated by Sagemaker Python SDK which are manual with boto3 or any other AWS SDKs (Java/Golang etc..)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0afaccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Customers need to package the code and upload the training code to S3 bucket using the boto3 S3 client s3://sagemaker-us-west-2-086613482928/huggingface-pytorch-training-2021-11-22-20-14-36-456/source/sourcedir.tar.gz\n",
    "# 2. Customers need to find the right training image name and populate it in the API along with other parameters as mentioned below. Most of the below parameters are populated by Sagemaker Python SDK automagically when using HuggingFaceEstimator() class and then using HuggingFaceEstimator.fit() which also finds the right training image\n",
    "# 3. Customers need to use a training waiter to wait for the training job to complete. Sagemaker python SDK automatically implements the waiter in the HuggingFaceEstimator.fit()\n",
    "# 4. Customers need to call 3 Sagemaker Hosting Cloud APIs to deploy a trained model and create an endpoint. These 3 APIs are automatically called by Sagemaker Python SDK using the HuggingFaceEstimator.deploy() method\n",
    "# 5. Customers need to find the right inference image to pass in the above APIs - Sagemaker Python SDK automagically finds the correct inference image when using HuggingFaceEstimator.deploy()\n",
    "# 6. Customers need to use an inference waiter to wait for the endpoint creation to complete. Sagemaker python SDK automatically implements the waiter in the HuggingFaceEstimator.deploy()\n",
    "# 7. Customers need to use Sagemaker runtime APIs to call prediction on the endpoint. This is automated by Sagemaker Python SDK using predictor.predict(). predictor is the object created by Sagemaker Python SDK returned after calling HuggingFaceEstimator.deploy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06a94e",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4c042",
   "metadata": {},
   "source": [
    "# Package the training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a36acdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "train.py\r\n"
     ]
    }
   ],
   "source": [
    "! cd scripts; tar -czvf ../sourcedir.tar.gz train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5c6673d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = \"huggingface-pytorch-training-manual-{}\".format(int(time.time()))\n",
    "\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('sourcedir.tar.gz', sess.default_bucket(), f'{training_job_name}/source/sourcedir.tar.gz')\n",
    "\n",
    "s3_code_directory = f's3://{sess.default_bucket()}/{training_job_name}/source/sourcedir.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b298a",
   "metadata": {},
   "source": [
    "# Create Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8077abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "response = client.create_training_job(\n",
    "    TrainingJobName=training_job_name,\n",
    "    HyperParameters={\n",
    "        \"epochs\": \"1\",\n",
    "        \"model_name\": '\"distilbert-base-uncased\"',\n",
    "        \"sagemaker_container_log_level\": \"20\",\n",
    "        \"sagemaker_job_name\": training_job_name,\n",
    "        \"sagemaker_program\": '\"train.py\"',\n",
    "        \"sagemaker_region\": '\"us-west-2\"',\n",
    "        \"sagemaker_submit_directory\": s3_code_directory,\n",
    "        \"train_batch_size\": \"32\"\n",
    "    },\n",
    "    AlgorithmSpecification={\n",
    "        'TrainingImage': '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.7-transformers4.6-gpu-py36-cu110-ubuntu18.04',\n",
    "        'TrainingInputMode': 'File',\n",
    "        'EnableSageMakerMetricsTimeSeries': True\n",
    "    },\n",
    "    RoleArn='arn:aws:iam::086613482928:role/service-role/AmazonSageMaker-ExecutionRole-20200122T153461',\n",
    "    InputDataConfig=[\n",
    "        {\n",
    "            'ChannelName': 'train',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': training_input_path,\n",
    "                    'S3DataDistributionType': 'FullyReplicated'\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'ChannelName': 'test',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': test_input_path,\n",
    "                    'S3DataDistributionType': 'FullyReplicated'\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    ],\n",
    "    OutputDataConfig={\n",
    "        'S3OutputPath': 's3://sagemaker-us-west-2-086613482928/'\n",
    "    },\n",
    "    ResourceConfig={\n",
    "        'InstanceType': 'ml.p3.8xlarge',\n",
    "        'InstanceCount': 1,\n",
    "        'VolumeSizeInGB': 30    },\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 86400    },\n",
    "    EnableNetworkIsolation=False,\n",
    "    EnableInterContainerTrafficEncryption=False,\n",
    "    EnableManagedSpotTraining=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aa5b6835",
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = client.get_waiter('training_job_completed_or_stopped')\n",
    "waiter.wait(\n",
    "    TrainingJobName=training_job_name,\n",
    "    WaiterConfig={\n",
    "        'Delay': 60\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad6f9b",
   "metadata": {},
   "source": [
    "# DEPLOY MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7472312",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "356f388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model-{}\".format(training_job_name)\n",
    "response = client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'ContainerHostname': 'Container1',\n",
    "        'Image': '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:1.7-transformers4.6-gpu-py36-cu110-ubuntu18.04',\n",
    "        'Mode': 'SingleModel',\n",
    "        'ModelDataUrl': 's3://sagemaker-us-west-2-086613482928/huggingface-pytorch-training-manual-9/output/model.tar.gz',\n",
    "        'Environment': {\n",
    "            'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
    "            'SAGEMAKER_REGION': 'us-west-2'\n",
    "        }\n",
    "    },\n",
    "    ExecutionRoleArn='arn:aws:iam::086613482928:role/service-role/AmazonSageMaker-ExecutionRole-20200122T153461'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa2885",
   "metadata": {},
   "source": [
    "# Create Endpoint Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "03f487a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"endpoint-config-{}\".format(model_name)\n",
    "endpoint_config_name = endpoint_config_name[:63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2888248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': model_name,\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.g4dn.xlarge',\n",
    "            'InitialVariantWeight': 1\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd956d",
   "metadata": {},
   "source": [
    "# Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "eff34ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"endpoint-{}\".format(endpoint_config_name)\n",
    "endpoint_name = endpoint_name[:63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "32dd33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1167e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(\n",
    "    EndpointName=endpoint_name,\n",
    "    WaiterConfig={\n",
    "        'Delay': 60\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eaed39",
   "metadata": {},
   "source": [
    "# RUN INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e40337",
   "metadata": {},
   "source": [
    "# Invoke Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d406e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '6a77dce7-373e-4000-80c4-12cb009b95b7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '6a77dce7-373e-4000-80c4-12cb009b95b7', 'x-amzn-invoked-production-variant': 'model-huggingface-pytorch-training-manual-1637715507', 'date': 'Wed, 24 Nov 2021 02:45:38 GMT', 'content-type': 'application/json', 'content-length': '48'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'model-huggingface-pytorch-training-manual-1637715507', 'Body': <botocore.response.StreamingBody object at 0x7fd5407a1e50>}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "sentiment_input= {\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(sentiment_input),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dd648e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9627289175987244}]\n"
     ]
    }
   ],
   "source": [
    "response_body = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "print(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc94bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p37",
   "language": "python",
   "name": "conda_pytorch_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
